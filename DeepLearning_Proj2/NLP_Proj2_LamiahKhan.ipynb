{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lamiah Khan: NLP Project 2**\n",
        "\n",
        "*  ***YOU DO NOT NEED TO RUN THE FOLLOWING ON COLAB!***: The dataset used for this project was corpus 1. The first step is to convert the textfiles to a csv file. Corpus training set will be train.csv, and the test set will be test.csv. My recommendation is to run this script outside of Colab, otherwise you would have to upload all the files from corpus 1 (including those in the folder). Personally, I ran this script on Anaconda Spyder. I am attaching the scripts I got outputted to the email. In case you want to try: here is the script!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T0b20m5hXP2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# link for reference code: https://www.geeksforgeeks.org/convert-text-file-to-csv-using-python-pandas/\n",
        "\n",
        "def load_data(article_path, label_file):\n",
        "    data = []\n",
        "    with open(label_file, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        file_path = os.path.join(article_path, parts[0])\n",
        "        label = parts[1]\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                text = file.read()\n",
        "            data.append({\"text\": text, \"label\": label})\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: The file {file_path} does not exist and will be skipped.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {e}\")\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def main():\n",
        "    base_path = \"./\"\n",
        "    train_label_file = \"corpus1_train.labels\"\n",
        "    test_label_file = \"corpus1_test.labels\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading training data...\")\n",
        "        train_data = load_data(base_path, train_label_file)\n",
        "        print(\"Loading testing data...\")\n",
        "        test_data = load_data(base_path, test_label_file)\n",
        "\n",
        "        train_data.to_csv(\"train.csv\", index=False)\n",
        "        test_data.to_csv(\"test.csv\", index=False)\n",
        "        print(\"Dataset saved to CSV format\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0y3wlSOqutE",
        "outputId": "29e3c0af-b8ac-4b6d-b04f-e165e979a7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "An error occurred: [Errno 2] No such file or directory: 'corpus1_train.labels'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1ST CODE BLOCK FOR RUNNING***: the following code block imports all the neccessary libraries, and gets device to GPU."
      ],
      "metadata": {
        "id": "3UYXpWtR5tOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Downloading necessary NLTK data if not already downloaded\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFqMkIqZ54ij",
        "outputId": "41be436d-c10e-40f1-9711-9a61627fdc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2ND CODE BLOCK FOR RUNNING***: Next, we will load and process all the files. In order to pre-process the files, I (1) removed stop words (2) removed white space (3) removed digits. Then to process the text, I (1) tokenized (2) created vocabulary (3) changed the labels into numerical format (4) converted to tensors. I also created a function to create word2vec embeddings for the experiment. I padded the word embeddings, but it is set to 0 since RNN on pytorch should have automatic padding."
      ],
      "metadata": {
        "id": "dAAfRNQW6Yya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the CSV files, and checking if it exists and if it doesn't then create it\n",
        "train_csv = 'train.csv'\n",
        "test_csv = 'test.csv'\n",
        "\n",
        "if not os.path.exists(train_csv):\n",
        "    print(\"Creating training CSV...\")\n",
        "    train_df = load_data('./', 'corpus1_train.labels')\n",
        "    train_df.to_csv(train_csv, index=False)\n",
        "else:\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "\n",
        "if not os.path.exists(test_csv):\n",
        "    print(\"Creating testing CSV...\")\n",
        "    test_df = load_data('./', 'corpus1_test.labels')\n",
        "    test_df.to_csv(test_csv, index=False)\n",
        "else:\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "print(\"Train dataset shape:\", train_df.shape)\n",
        "print(\"Test dataset shape:\", test_df.shape)\n",
        "\n",
        "\n",
        "# Preprocessing function to clean and tokenize text\n",
        "# A lot of this is imported from Project 1\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and whitespace\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.strip()]\n",
        "    return tokens\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['encoded_label'] = label_encoder.fit_transform(train_df['label'])\n",
        "\n",
        "# Create vocabulary based on preprocessed training text\n",
        "all_tokens = [token for text in train_df['text'] for token in preprocess_text(text)]\n",
        "vocab = sorted(set(all_tokens))\n",
        "word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}  # Reserve 0 for padding\n",
        "word_to_idx['<PAD>'] = 0\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "print(\"Vocabulary size:\", len(word_to_idx))\n",
        "\n",
        "# Function to create Word2Vec embeddings\n",
        "def create_word2vec_embeddings(texts, embed_dim):\n",
        "    sentences = [preprocess_text(text) for text in texts]\n",
        "    model = Word2Vec(sentences, vector_size=embed_dim, window=5, min_count=1, workers=4)\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFySupM97JRY",
        "outputId": "a3229e02-24c6-46ee-caeb-9abe42d22ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: (894, 2)\n",
            "Test dataset shape: (403, 2)\n",
            "Vocabulary size: 4656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3RD CODE BLOCK FOR RUNNING***:\n",
        "The TextDataset class is a custom dataset for handling text data in PyTorch, designed to:\n",
        "* Initialize with text data and labels (optional) and additional parameters, including word_to_idx (for word-to-index mapping), max_len (maximum text length), and word2vec_model (for embeddings).\n",
        "* Return the dataset length with __len__ to provide the number of samples.\n",
        "* Retrieve preprocessed text with __getitem__, encoding text into word indices or embeddings, then padding or truncating to max_len.\n",
        "\n",
        "In __getitem__:\n",
        "* If word2vec_model is provided, each token is represented by a Word2Vec vector if available, or a zero-vector if not.\n",
        "* If word2vec_model is not provided, each token is encoded by looking up its index in word_to_idx.\n",
        "Padding is added if the text is shorter than max_len, and if longer, it is truncated.\n",
        "* The function returns the encoded text (as a tensor) and, if available, the label as a tensor.\n"
      ],
      "metadata": {
        "id": "Kt5XzZfs7jjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# References: https://discuss.huggingface.co/t/help-understanding-how-to-build-a-dataset-for-language-as-with-the-old-textdataset/5870\n",
        "# https://github.com/huggingface/transformers/issues/24742\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels=None, word_to_idx=None, max_len=50, word2vec_model=None, use_static_embedding=False):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.max_len = max_len\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.use_static_embedding = use_static_embedding\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]\n",
        "        tokens = preprocess_text(text)\n",
        "        if self.use_static_embedding:\n",
        "            # Convert tokens to embeddings directly\n",
        "            encoded_text = [self.word2vec_model.wv[token] if token in self.word2vec_model.wv else np.zeros(self.word2vec_model.vector_size) for token in tokens]\n",
        "            # Pad or truncate to `max_len`\n",
        "            if len(encoded_text) < self.max_len:\n",
        "                encoded_text.extend([np.zeros(self.word2vec_model.vector_size)] * (self.max_len - len(encoded_text)))\n",
        "            else:\n",
        "                encoded_text = encoded_text[:self.max_len]\n",
        "            encoded_text = np.array(encoded_text, dtype=np.float32)  # Ensure float32 type\n",
        "        else:\n",
        "            # Use indices for dynamic embeddings\n",
        "            encoded_text = [self.word_to_idx.get(token, 0) for token in tokens]\n",
        "            if len(encoded_text) < self.max_len:\n",
        "                encoded_text += [0] * (self.max_len - len(encoded_text))\n",
        "            else:\n",
        "                encoded_text = encoded_text[:self.max_len]\n",
        "            encoded_text = np.array(encoded_text, dtype=np.int64)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label = self.labels.iloc[idx]\n",
        "            return torch.tensor(encoded_text), torch.tensor(label)\n",
        "        else:\n",
        "            return torch.tensor(encoded_text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n"
      ],
      "metadata": {
        "id": "EFKcUeJ49Ud9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4TH CODE BLOCK FOR RUNNING***: The TextClassifier class defines the neural network architecture for text classification.\n",
        "\n",
        "For Initialization:\n",
        "* Embedding Layer: Either uses pretrained embeddings (static) or trains embeddings (non-static).\n",
        "* RNN Layer: Initializes a recurrent neural network (LSTM, GRU, or RNN) based on input parameters:\n",
        "* rnn_type specifies the type (LSTM/GRU/RNN).\n",
        "* bidirectional allows for bidirectional RNNs, enabling processing both forward and backward.\n",
        "* num_layers specifies the number of stacked RNN layers.\n",
        "dropout applies dropout for regularization between layers if num_layers > 1.\n",
        "* A dropout layer and fully connected layer follow the RNN, mapping to the final output classes.\n",
        "\n",
        "There is also Forward Pass, which:\n",
        "* Embeds the input if using non-static embedding, else uses the input directly (already a Word2Vec vector).\n",
        "* Processes the embedded sequence through the RNN.\n",
        "* Uses only the final RNN hidden state for prediction.\n",
        "* Applies dropout and then outputs the final class scores."
      ],
      "metadata": {
        "id": "9c7d3hSH9iue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define text classifier model: https://github.com/claravania/lstm-pytorch/blob/master/model.py\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, rnn_type='LSTM', bidirectional=False, num_layers=1, dropout=0.5, use_static_embedding=True, word2vec_weights=None):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.use_static_embedding = use_static_embedding\n",
        "        if use_static_embedding:\n",
        "            self.embedding = nn.Embedding.from_pretrained(word2vec_weights, freeze=True)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "# Initialize RNN (LSTM/GRU/RNN) for later experimentation: https://discuss.pytorch.org/t/bidirectional-3-layer-lstm-hidden-output/41336\n",
        "        self.rnn_type = rnn_type\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
        "                               bidirectional=bidirectional, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers,\n",
        "                              bidirectional=bidirectional, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers,\n",
        "                              bidirectional=bidirectional, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        direction_multiplier = 2 if bidirectional else 1\n",
        "        self.fc = nn.Linear(hidden_dim * direction_multiplier, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.use_static_embedding:\n",
        "            embedded = self.embedding(x)\n",
        "        else:\n",
        "            embedded = x\n",
        "\n",
        "        if self.rnn_type in ['LSTM', 'GRU']:\n",
        "            rnn_out, _ = self.rnn(embedded)\n",
        "        else:\n",
        "            rnn_out, _ = self.rnn(embedded)\n",
        "\n",
        "        final_hidden_state = rnn_out[:, -1, :]\n",
        "        dropped = self.dropout(final_hidden_state)\n",
        "        return self.fc(dropped)"
      ],
      "metadata": {
        "id": "wXbXXM4P-LaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***5TH CODE BLOCK FOR RUNNING***: Next, we have the training function. The train_model function trains the TextClassifier model.\n",
        "\n",
        "Training Loop:\n",
        "* For each epoch, it iterates over batches, computing the model’s output, loss, and performing backpropagation to update weights.\n",
        "train_loss accumulates the loss per epoch.\n",
        "\n",
        "Validation Phase:\n",
        "* After each epoch, it switches to evaluation mode to calculate the validation loss and accuracy without gradient updates.\n",
        "* Predicts classes, compares with true labels, and calculates accuracy.\n"
      ],
      "metadata": {
        "id": "PGUYFPFu-WJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
        "\n",
        "    epoch_accuracies = []  # Store accuracy for each epoch\n",
        "    # code reference: https://stackoverflow.com/questions/51503851/calculate-the-accuracy-every-epoch-in-pytorch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            texts, labels = batch\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(texts)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase (after each epoch)\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():  # No gradient computation for validation\n",
        "            for batch in val_loader:\n",
        "                texts, labels = batch\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "                output = model(texts)\n",
        "                loss = criterion(output, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Get predictions and calculate accuracy\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        epoch_accuracies.append(epoch_accuracy)\n",
        "\n",
        "                # Print concise output for the epoch\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
        "              f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
        "              f\"Val Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    return epoch_accuracies"
      ],
      "metadata": {
        "id": "zErmYOMH-mIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***6TH CODE BLOCK FOR RUNNING***: The evaluate_accuracy function evaluates the trained model on test data by:\n",
        "* Switching to evaluation mode, iterating over batches of test data, and accumulating the total number of samples processed.This is used to assess generalization to unseen data."
      ],
      "metadata": {
        "id": "Mup2Q7oJ-0AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function (testing accuracy)\n",
        "def evaluate_accuracy(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts in test_loader:\n",
        "            texts = texts.to(device)\n",
        "            outputs = model(texts)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += texts.size(0)\n",
        "    return total"
      ],
      "metadata": {
        "id": "SjYq7lQe_Aqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***7TH CODE BLOCK FOR RUNNING***: # https://github.com/keras-team/keras/issues/853 Next, we defined loaders and definitions so we can run experiments. The parameters I worked with included:\n",
        "* embed_dim: Dimension of the word embeddings.\n",
        "* hidden_dim: Dimension of the RNN’s hidden layer.\n",
        "* lr: Learning rate for the optimizer.\n",
        "* epochs: Number of epochs to train the model.\n",
        "* rnn_type: Type of recurrent layer to use (options: LSTM, GRU, RNN).\n",
        "* bidirectional: If True, the RNN layer will be bidirectional.\n",
        "* num_layers: Number of stacked RNN layers.\n",
        "* dropout: Dropout rate to apply after the RNN layers.\n",
        "* use_static_embedding: If True, uses static (pre-trained) embeddings; otherwise, trainable embeddings.\n",
        "\n",
        "I loaded word embeddings (is using static embeddings:\n",
        "* If use_static_embedding=True, it calls create_word2vec_embeddings to generate embeddings based on the training text data.\n",
        "* Each word in word_to_idx is converted into its Word2Vec embedding if available, or a zero vector if not found in word2vec_model.\n",
        "* word2vec_weights stores the pre-trained Word2Vec embeddings in a PyTorch tensor. This tensor will be used in the TextClassifier as a fixed (non-trainable) embedding layer.\n",
        "* If use_static_embedding=False, the word2vec_weights is set to None, and a new embedding layer will be learned from scratch during training.\n",
        "\n",
        "I also create a TextDataset for training, validation, and test datasets:\n",
        "* If using static embeddings, the word2vec_model is passed into TextDataset, which will use Word2Vec embeddings for each word.\n",
        "* Otherwise, word_to_idx is used for encoding the text into word indices.\n",
        "* DataLoader instances are then created for each dataset, batching the data for efficient model training and evaluation.\n",
        "\n",
        "For the TextCalssifier:\n",
        "word2vec_weights is provided if use_static_embedding=True, and use_static_embedding is set accordingly. If use_static_embedding=False, the model will initialize and learn its own embedding weights."
      ],
      "metadata": {
        "id": "K1yEOX4i_Fzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(embed_dim, hidden_dim, lr, epochs, rnn_type='LSTM', bidirectional=False, num_layers=1, dropout=0.5, use_static_embedding=True):\n",
        "  # reference code: https://github.com/ultralytics/yolov5/blob/master/classify/train.py\n",
        "\n",
        "    if use_static_embedding:\n",
        "        word2vec_model = create_word2vec_embeddings(train_df['text'], embed_dim)\n",
        "        word_to_idx = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.key_to_index)}\n",
        "        word_to_idx['<PAD>'] = 0\n",
        "        idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "        train_dataset = TextDataset(train_texts, train_labels, word_to_idx=word_to_idx, word2vec_model=word2vec_model, use_static_embedding=True)\n",
        "        val_dataset = TextDataset(val_texts, val_labels, word_to_idx=word_to_idx, word2vec_model=word2vec_model, use_static_embedding=True)\n",
        "        test_dataset = TextDataset(test_df['text'], word_to_idx=word_to_idx, word2vec_model=word2vec_model, use_static_embedding=True)\n",
        "\n",
        "        # Pre-load Word2Vec weights\n",
        "        word2vec_weights = torch.FloatTensor([word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(embed_dim) for word in word_to_idx]).to(device)\n",
        "    else:\n",
        "        # Handle dynamic embedding case\n",
        "        all_tokens = [token for text in train_df['text'] for token in preprocess_text(text)]\n",
        "        vocab = sorted(set(all_tokens))\n",
        "        word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
        "        word_to_idx['<PAD>'] = 0\n",
        "        idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "        word2vec_weights = None  # No pre-trained weights in this case\n",
        "\n",
        "        train_dataset = TextDataset(train_texts, train_labels, word_to_idx=word_to_idx, use_static_embedding=False)\n",
        "        val_dataset = TextDataset(val_texts, val_labels, word_to_idx=word_to_idx, use_static_embedding=False)\n",
        "        test_dataset = TextDataset(test_df['text'], word_to_idx=word_to_idx, use_static_embedding=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    model = TextClassifier(len(word_to_idx), embed_dim, hidden_dim, len(label_encoder.classes_),\n",
        "                           rnn_type=rnn_type, bidirectional=bidirectional, num_layers=num_layers,\n",
        "                           dropout=dropout, use_static_embedding=use_static_embedding,\n",
        "                           word2vec_weights=word2vec_weights).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    epoch_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "    avg_accuracy = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "    print(f\"Average Accuracy: {avg_accuracy:.2f}%\")\n",
        "\n",
        "    total_samples = evaluate_accuracy(model, test_loader)\n",
        "    return total_samples, avg_accuracy\n"
      ],
      "metadata": {
        "id": "xDA9WXL2A9G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***8TH CODE BLOCK FOR RUNNING***: This is the block where I ran experiments. FINAL ARCHITECTURE THAT WORKS BEST HAS BEEN LEFT UNCOMMENTED! I compared vanilla RNNs to LSTMs, and even to GRUs, which was not talked about explicitly in class, but I found they also worked with high accuracy! I also compared single-direction LSTMs to bi-LSTMs, and stacked two or three layers of LSTMs together. I also experiment with hyperparameters, and I also experimented with the system learning an embedding layer versus using static embedding with word2vec. Overall, I had the best results for Bidirectional LSTM with four layers and no static word embedding. I ran this three times, and always got an overall accuracy between 72.18% to 75.61% (as shown in the write-up). Bidirectional GRU with 1 layer also resulted in a 78.07% accuracy."
      ],
      "metadata": {
        "id": "sFa5gA1iB4tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_df['text'], train_df['encoded_label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Running some experiments with different configurations: FINAL CHOICE IS NOT COMMENTED\n",
        "experiments = [\n",
        "    # Vanilla RNN\n",
        "    #(100, 128, 0.001, 20, 'RNN', False, 1, 0.5, False),\n",
        "\n",
        "    # Increased hidden dimension\n",
        "    #(100, 256, 0.001, 20, 'LSTM', False, 2, 0.4, True),  # LSTM with more hidden units, static embedding\n",
        "    #(100, 256, 0.0005, 30, 'GRU', True, 2, 0.4, False),  # GRU, bidirectional, smaller lr\n",
        "\n",
        "    # More layers\n",
        "    (100, 128, 0.0005, 30, 'LSTM', True, 4, 0.3, False),  # LSTM, more layers, smaller lr\n",
        "    #(100, 128, 0.0005, 30, 'LSTM', True, 4, 0.3, True),  # LSTM, more layers, smaller lr\n",
        "    #(200, 128, 0.0005, 40, 'GRU', False, 3, 0.3, True),   # GRU, increased embed_dim, longer epochs\n",
        "\n",
        "    # Higher embedding dimensions\n",
        "    #(300, 128, 0.001, 20, 'LSTM', True, 2, 0.4, True),    # Higher embed_dim with LSTM, bidirectional\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    #(200, 256, 0.0005, 30, 'LSTM', True, 2, 0.6, False),\n",
        "\n",
        "    # Lower dropout\n",
        "    #(100, 128, 0.001, 20, 'RNN', True, 1, 0.3, False),    # Vanilla RNN, lower dropout, bidirectional\n",
        "    #(100, 512, 0.0005, 40, 'GRU', True, 2, 0.3, True),    # Higher hidden dimension, longer epochs, bidirectional\n",
        "\n",
        "    # Experiment with small learning rate and more epochs\n",
        "    #(200, 256, 0.0001, 40, 'LSTM', True, 3, 0.4, True),   # Lower lr with LSTM, more layers, static embedding\n",
        "]\n",
        "\n",
        "\n",
        "# Running each experiment and storing the results\n",
        "results = []\n",
        "for params in experiments:\n",
        "    total_samples, avg_accuracy = run_experiment(*params)\n",
        "    results.append((params, total_samples, avg_accuracy))\n",
        "\n",
        "print(\"\\nExperiment Results:\")\n",
        "for params, total_samples, avg_accuracy in results:\n",
        "    print(f\"Model: {params[4]}, Bidirectional: {params[5]}, Layers: {params[6]}, \"\n",
        "          f\"Embed dim: {params[0]}, Hidden dim: {params[1]}, LR: {params[2]}, \"\n",
        "          f\"Epochs: {params[3]}, Dropout: {params[7]}, Static Embedding: {params[8]}\")\n",
        "    print(f\"Total samples processed: {total_samples}\")\n",
        "    print(f\"Average Accuracy across all epochs: {avg_accuracy:.2f}%\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdc9w_0kCl34",
        "outputId": "1a760dd5-68e3-4361-95fe-046ceb887c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 0.6346, Val Loss: 0.6218, Val Accuracy: 67.60%\n",
            "Epoch 2/30, Train Loss: 0.6106, Val Loss: 0.6279, Val Accuracy: 67.60%\n",
            "Epoch 3/30, Train Loss: 0.6190, Val Loss: 0.6190, Val Accuracy: 67.60%\n",
            "Epoch 4/30, Train Loss: 0.6201, Val Loss: 0.6186, Val Accuracy: 67.60%\n",
            "Epoch 5/30, Train Loss: 0.6173, Val Loss: 0.6186, Val Accuracy: 67.60%\n",
            "Epoch 6/30, Train Loss: 0.6121, Val Loss: 0.6203, Val Accuracy: 67.60%\n",
            "Epoch 7/30, Train Loss: 0.6123, Val Loss: 0.6201, Val Accuracy: 67.60%\n",
            "Epoch 8/30, Train Loss: 0.6111, Val Loss: 0.6168, Val Accuracy: 68.16%\n",
            "Epoch 9/30, Train Loss: 0.6178, Val Loss: 0.6054, Val Accuracy: 69.27%\n",
            "Epoch 10/30, Train Loss: 0.6003, Val Loss: 0.5997, Val Accuracy: 69.83%\n",
            "Epoch 11/30, Train Loss: 0.5952, Val Loss: 0.7101, Val Accuracy: 51.96%\n",
            "Epoch 12/30, Train Loss: 0.6224, Val Loss: 0.6080, Val Accuracy: 69.83%\n",
            "Epoch 13/30, Train Loss: 0.5781, Val Loss: 0.5659, Val Accuracy: 70.95%\n",
            "Epoch 14/30, Train Loss: 0.5370, Val Loss: 0.5807, Val Accuracy: 72.07%\n",
            "Epoch 15/30, Train Loss: 0.5280, Val Loss: 0.5179, Val Accuracy: 78.21%\n",
            "Epoch 16/30, Train Loss: 0.4964, Val Loss: 0.5362, Val Accuracy: 74.30%\n",
            "Epoch 17/30, Train Loss: 0.4320, Val Loss: 0.5176, Val Accuracy: 74.86%\n",
            "Epoch 18/30, Train Loss: 0.3875, Val Loss: 0.4991, Val Accuracy: 78.21%\n",
            "Epoch 19/30, Train Loss: 0.3834, Val Loss: 0.5571, Val Accuracy: 77.09%\n",
            "Epoch 20/30, Train Loss: 0.3377, Val Loss: 0.5048, Val Accuracy: 78.21%\n",
            "Epoch 21/30, Train Loss: 0.2948, Val Loss: 0.5061, Val Accuracy: 81.01%\n",
            "Epoch 22/30, Train Loss: 0.2466, Val Loss: 0.5012, Val Accuracy: 82.68%\n",
            "Epoch 23/30, Train Loss: 0.2458, Val Loss: 0.5395, Val Accuracy: 81.01%\n",
            "Epoch 24/30, Train Loss: 0.2416, Val Loss: 0.5267, Val Accuracy: 79.89%\n",
            "Epoch 25/30, Train Loss: 0.2044, Val Loss: 0.5773, Val Accuracy: 79.33%\n",
            "Epoch 26/30, Train Loss: 0.2360, Val Loss: 0.5323, Val Accuracy: 80.45%\n",
            "Epoch 27/30, Train Loss: 0.1914, Val Loss: 0.6134, Val Accuracy: 79.33%\n",
            "Epoch 28/30, Train Loss: 0.1624, Val Loss: 0.6095, Val Accuracy: 78.77%\n",
            "Epoch 29/30, Train Loss: 0.2015, Val Loss: 0.7816, Val Accuracy: 80.45%\n",
            "Epoch 30/30, Train Loss: 0.2583, Val Loss: 0.5397, Val Accuracy: 78.77%\n",
            "Average Accuracy: 73.59%\n",
            "\n",
            "Experiment Results:\n",
            "Model: LSTM, Bidirectional: True, Layers: 4, Embed dim: 100, Hidden dim: 128, LR: 0.0005, Epochs: 30, Dropout: 0.3, Static Embedding: False\n",
            "Total samples processed: 403\n",
            "Average Accuracy across all epochs: 73.59%\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
